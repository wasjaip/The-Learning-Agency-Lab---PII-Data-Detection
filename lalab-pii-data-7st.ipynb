{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7784594,"sourceType":"datasetVersion","datasetId":4555967},{"sourceId":7803679,"sourceType":"datasetVersion","datasetId":4340749},{"sourceId":7908837,"sourceType":"datasetVersion","datasetId":4646023}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"papermill":{"default_parameters":{},"duration":185.201765,"end_time":"2024-03-16T07:35:37.034723","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-16T07:32:31.832958","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1f29f01ec214468d969def388524a096":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34b83f3809864c1c9a10d3ad2aaddb85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5ac1d13c0a5410b98b707fc2a1b7f9b","placeholder":"​","style":"IPY_MODEL_c93b6e0c276c48c08e75209a8567189c","value":" 5/5 [00:00&lt;00:00, 55.37ex/s]"}},"5098d06c08ce43b0bc4be76386c31d9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51ad7e6068e6492badbc5057f33a804f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60a10c81fddc4c12b5083aee607949d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7dce13f4108647e188373b226f9c02b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc1fc33c57424076ac4102ac91acbede","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7c7387f3cea4dd49b1306125f4a81b6","value":5}},"9789fa1ac203430081a69c921b1a03a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a047ad23834a4074a0489cf5c8261f77","IPY_MODEL_7dce13f4108647e188373b226f9c02b0","IPY_MODEL_34b83f3809864c1c9a10d3ad2aaddb85"],"layout":"IPY_MODEL_de320fca73924a898c9c17f9225e0645"}},"a047ad23834a4074a0489cf5c8261f77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a148d86d9b59437e91d1f187c4cd37f2","placeholder":"​","style":"IPY_MODEL_51ad7e6068e6492badbc5057f33a804f","value":"#0: 100%"}},"a148d86d9b59437e91d1f187c4cd37f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a371a0214a0f4fc3ae9c15f583260eb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abe80b4df3e44200882f539211994b71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5ac1d13c0a5410b98b707fc2a1b7f9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c93b6e0c276c48c08e75209a8567189c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce7dd179917241c188b4b27dea3a0f36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dac460432a3e43cab813e887444da391","placeholder":"​","style":"IPY_MODEL_a371a0214a0f4fc3ae9c15f583260eb2","value":"#1: 100%"}},"d2ba9b51ca9a436893912a1b7d43c783":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abe80b4df3e44200882f539211994b71","placeholder":"​","style":"IPY_MODEL_1f29f01ec214468d969def388524a096","value":" 5/5 [00:00&lt;00:00, 59.67ex/s]"}},"d7c7387f3cea4dd49b1306125f4a81b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dac460432a3e43cab813e887444da391":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc49c7b765d64aa9bc04c35c4b1210be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de320fca73924a898c9c17f9225e0645":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e232abeff34a44d790d5410b3bf9de27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc49c7b765d64aa9bc04c35c4b1210be","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60a10c81fddc4c12b5083aee607949d8","value":5}},"fc1fc33c57424076ac4102ac91acbede":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffe97bd9fa2c43e1a46142026600e209":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce7dd179917241c188b4b27dea3a0f36","IPY_MODEL_e232abeff34a44d790d5410b3bf9de27","IPY_MODEL_d2ba9b51ca9a436893912a1b7d43c783"],"layout":"IPY_MODEL_5098d06c08ce43b0bc4be76386c31d9d"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/wasjaip/lalab-pii-data-7st?scriptVersionId=176385376\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# The Learning Agency Lab - PII Data Detection","metadata":{"papermill":{"duration":0.00678,"end_time":"2024-03-16T07:32:34.531354","exception":false,"start_time":"2024-03-16T07:32:34.524574","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import re\nimport os\nimport time\n\nimport numpy as np \nimport pandas as pd \n\nimport json\nimport argparse\nfrom itertools import chain\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nfrom colorama import Fore, Style\nfrom torch.utils.data import DataLoader\n\n\nimport gc\nimport torch\nimport numpy as np\n\nfrom scipy.special import softmax","metadata":{"_cell_guid":"bee4a697-dd01-403d-b03d-a8c11094a4c6","_uuid":"16395033-a39e-451f-84ca-3836ef66fce5","papermill":{"duration":18.2679,"end_time":"2024-03-16T07:32:52.805515","exception":false,"start_time":"2024-03-16T07:32:34.537615","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Настроечные параметры","metadata":{}},{"cell_type":"code","source":"# Ставим is_local в True, если локально работаем, если сабмитим - ставим в False\nis_local = False\n#is_local = True\n\n# Ставим в is_make_predition True если нужно сделать предсказания на инференсе и записать их на диск\n# Иначе пользуеся записанными ранее на диск предсказаниями\nis_make_predition = False\n#is_make_predition = True\n\n# Нужно печатать детали или только итог\n#is_print_details = False\nis_print_details = True\n\ntrain_Length = 10000 # Число текстов, которые возбмем для тестированя из тренировочных данных\n#train_Length = 100 # Число текстов, которые возбмем для тестированя из тренировочных данных\n\n# Число документов которые передаются в одно предсказание\n# и сохраняются на диск\nmax_batch_size = 500\n#max_batch_size = 20\n\n# Максимальная площать токенов на число документов при предсказании в батче\n# max_pred_sqaure = 40000*2000*3000\n# max_pred_sqaure = 40000*2000*3000*10000\n\n# Лимит времени на выполенния предсказаний моделями по всем документам\n#all_docs_time_limit = (8*60*60+60*30)\nall_docs_time_limit = (5*60*60)\n#all_docs_time_limit = (40*2)\n\n# Лимит времени на выполенния предсказаний моделями по коротким документам\nshort_docs_time_limit = (8*60*60+60*30)\n#short_docs_time_limit = (40*2)\n\n# Лимит времени на выполенния предсказаний моделями по документам c pii\n#pii_docs_time_limit = (8*60*60+60*30)\n#pii_docs_time_limit = (40*3)\n\n# Лимит времени на выполенния предсказаний моделями по документам c сомнениями\n#doubts_docs_time_limit = (8*60*60+60*30)\n#doubts_docs_time_limit = (40*1)\n\n# range_share Доля документов которые обработаем.\n# Обрабатываем сначала с более коротких документов\nrange_share = 2/3\n#range_share = 9/10\n\n# Порог вероятности метки O для финального предсказания\n#all_docs_threshold = 0.9735\nfinal_docs_threshold = 0.93\n\n# Порог вероятности метки O для поиска документов с pii\n# Это предварительный порог. Чтобы только найти документы в которых потом еще искать\n# Персональную информацию\n#all_docs_threshold = 0.9735\nfind_pii_docs_threshold = 0.93\n\n# Пороги раздельно по классам\nthresholds_unique = [0.15, #'B-EMAIL',\n                     0.15, #'B-ID_NUM',\n                     0.10, #'B-NAME_STUDENT',\n                     0.15, #'B-PHONE_NUM',\n                     0.15, #'B-STREET_ADDRESS',\n                     0.15, #'B-URL_PERSONAL',\n                     0.15, #'B-USERNAME',\n                     0.15, #'I-ID_NUM',\n                     0.10, #'I-NAME_STUDENT',\n                     0.15, #'I-PHONE_NUM',\n                     0.15, #'I-STREET_ADDRESS',\n                     0.15, #'I-URL_PERSONAL',\n                     0]    #'O'\n# Пороги раздельно по классам","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Время старта работы ноутбука\nnotebook_starttime = time.time()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Возвращает сколько уже работает ноутбук\ndef p_time():\n    #\n    run_time = round(time.time() - notebook_starttime)\n    return str(run_time).zfill(5)+' sec:'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(p_time(), 'start notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание меток токенов","metadata":{}},{"cell_type":"markdown","source":"### Предсказанием меток токенов трансформерами.","metadata":{}},{"cell_type":"code","source":"# Максимальная длина текста для вывода\n# INFERENCE_MAX_LENGTH = 4500\n\ndef tokenize(example, tokenizer):\n    \n    # Максимальная длина текста для вывода\n    INFERENCE_MAX_LENGTH = 3500\n    text = [] # Инициализация списка для хранения текста\n    token_map = [] # Инициализация списка для сопоставления индексов токенов\n    \n    idx = 0 # Начальный индекс для отслеживания токенов\n    \n    # Перебор токенов и связанных с ними пробелов после токенов\n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        text.append(t) # Добавление каждого токена в список 'text'\n        \n        # Расширение 'token_map' текущим индексом, повторенным столько раз, сколько символов в токене\n        token_map.extend([idx] * len(t))\n        \n        # Добавление пробела для следующего пробела и отметка его '-1' в 'token_map'\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1 # Увеличение 'idx' для следующего токена\n        \n    # Токенизация объединенного текста 'text' и возвращение смещений вместе с 'token_map'\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    \n    # Возвращение словаря, содержащего токенизированные данные и 'token_map'\n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n","metadata":{"papermill":{"duration":0.021066,"end_time":"2024-03-16T07:32:55.510521","exception":false,"start_time":"2024-03-16T07:32:55.489455","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загрузка данных из файла JSON\n\n# Локально работаем с тренировочной выборкой\n# При сабмите загружаекм тестовую выборку\ndata_path = ('/kaggle/input/pii-detection-removal-from-educational-data/train.json' if is_local\n             else '/kaggle/input/pii-detection-removal-from-educational-data/test.json')\ntest_data = json.load(open(data_path))\n\n# Сортировка test_data по числу токенов в списке tokens каждого элемента\n# Предположительно это до должно ускорить инференс за счет более ровного размера тензоров в батче\ntest_data = sorted(test_data, key=lambda x: len(x['tokens']))\n\nif is_local:\n    # Возьмем для проверки не все тренировочные данные. А только часть. train_Length\n    test_data = test_data[0:train_Length]\n\nif is_local:\n    # Создадим отдельный датафрейм df_result дли исхходных размеченных данных.\n    # Чтобы потом сравнить какие персональные данные были найдены, а какие нет\n    # А может что лишнего был онайдено\n    df_result = pd.DataFrame({\n        \"document\": [x[\"document\"] for x in test_data], # Получение номеров документов\n        \"tokens\": [x[\"tokens\"] for x in test_data], # Получение токенов\n        \"true_labels\": [x[\"labels\"] for x in test_data], # Получение меток токенов\n        \"pred_labels\": [['O'] * len(x[\"labels\"]) for x in test_data], # Пока все предсказанные метки заполняем 'О'. Так как еще не нашли персональную информацию\n        \"pred_ind\": [[-1] * len(x[\"labels\"]) for x in test_data], # Индекс соответствующего токена в предсказании\n    })\n    \n\n'''\nmodel_paths = {\n    '/kaggle/input/37vp4pjt': 10,\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 2,\n    '/kaggle/input/pii-deberta-models/cola del piinguuino' : 1,\n    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 5,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha': 3,\n    '/kaggle/input/pii-deberta-models/cola-de-piiranha':1,\n    '/kaggle/input/pii-models/piidd-org-sakura': 2,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-persuade_v0':1,\n}\n'''\n#'/kaggle/input/pii-v3-hf':1,\nmodel_paths = {\n    #'/kaggle/input/pii-v3-hf':1,\n    '/kaggle/input/37vp4pjt': 5,\n    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 2,\n    '/kaggle/input/pii-deberta-models/cola del piinguuino': 2,\n    '/kaggle/input/pii-deberta-models/cola-de-piiranha': 2,\n    '/kaggle/input/pii-models/piidd-org-sakura': 2,\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 3,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha': 2,\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-persuade_v0': 1,\n    # Дополнительные модели и пути:\n    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-mixtral-v1': 1,\n    '/kaggle/input/pii-deberta-models/cola-de-piiranha-mixtral-v1': 1,\n    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha-mixtral-v1': 1,\n    '/kaggle/input/pii-deberta-models/piiranha': 1, \n    '/kaggle/input/pii-deberta-models/piranha': 1,  \n    '/kaggle/input/pii-deberta-models/cuerpo del piinguuino': 1,\n}\n\n\n\n# Выбор первого пути модели из словаря\nfirst_model_path = list(model_paths.keys())[0]\n\n# Инициализация токенизатора из первой модели\ntokenizer = AutoTokenizer.from_pretrained(first_model_path)\n","metadata":{"_cell_guid":"f711426b-b27b-43ee-a75a-7de94b15b486","_uuid":"8ab59eff-8c6f-48ef-ab4d-ee25a1705d2d","papermill":{"duration":1.31632,"end_time":"2024-03-16T07:32:56.833159","exception":false,"start_time":"2024-03-16T07:32:55.516839","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Вычисляем общий вес\ntotal_weight = sum(model_paths.values())\n\n# Список документов\ndoc_list = [x[\"document\"] for x in test_data]\n\n# Директория для сохранения промежуточных предсказаний всех моделей\nintermediate_dir = './model_predictions'\nos.makedirs(intermediate_dir, exist_ok=True)  # Создаем директорию, если она не существует\n\n# Директория для сохранения промежуточных предсказаний текущей модели\npredict_dir = './predict_dir'\nos.makedirs(predict_dir, exist_ok=True)  # Создаем директорию, если она не существует\n\nconfig = json.load(open(Path(first_model_path) / \"config.json\"))  # Загружаем конфигурационный файл модели\n#! id2label = config[\"id2label\"]  # Извлекаем словарь для преобразования id в метки\nid2label = {'0': 'B-EMAIL',\n            '1': 'B-ID_NUM',\n            '2': 'B-NAME_STUDENT',\n            '3': 'B-PHONE_NUM',\n            '4': 'B-STREET_ADDRESS',\n            '5': 'B-URL_PERSONAL',\n            '6': 'B-USERNAME',\n            '7': 'I-ID_NUM',\n            '8': 'I-NAME_STUDENT',\n            '9': 'I-PHONE_NUM',\n            '10': 'I-STREET_ADDRESS',\n            '11': 'I-URL_PERSONAL',\n            '12': 'O'}\n\nweights = list(model_paths.values())\nweights","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Делает датасет для обучения\n# docs_to_process - список указывающий обрабатывать документ или нет.\n# если стоит 1 - обрабатывать, если 0 - пропускаем документ и не включаем в датасет\ndef make_dataset(docs_to_process):\n    global ds, ds_tokens_len\n    # Создание набора данных из загруженных данных\n    if 'ds'in globals():\n        del ds\n    ds = Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for idx, x in enumerate(test_data) if docs_to_process[idx] == 1], # Получение текста из каждого элемента\n        \"document\": [x[\"document\"] for idx, x in enumerate(test_data) if docs_to_process[idx] == 1], # Получение номеров документов\n        \"tokens\": [x[\"tokens\"] for idx, x in enumerate(test_data) if docs_to_process[idx] == 1], # Получение токенов\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for idx, x in enumerate(test_data) if docs_to_process[idx] == 1], # Получение информации о пробелах\n    })\n    # Токенизация набора данных с использованием функции 'tokenize' в параллельном режиме\n    ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)\n    ds_tokens_len = [len(x[\"tokens\"]) for idx, x in enumerate(test_data) if docs_to_process[idx] == 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Возвращает только ту часть предсказаний, которые указаны в docs_to_process\ndef get_pred_part(predictions, docs_to_process):\n    # Создаем маску, чтобы найти индексы строк, которые нужно оставить\n    indices_to_keep = np.where(np.array(docs_to_process) == 1)[0]\n    return predictions[indices_to_keep]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_convert_list():\n    standart_label2id ={\n        'B-EMAIL': 0,\n        'B-ID_NUM': 1,\n        'B-NAME_STUDENT': 2,\n        'B-PHONE_NUM': 3,\n        'B-STREET_ADDRESS': 4,\n        'B-URL_PERSONAL': 5,\n        'B-USERNAME': 6,\n        'I-ID_NUM': 7,\n        'I-NAME_STUDENT': 8,\n        'I-PHONE_NUM': 9,\n        'I-STREET_ADDRESS': 10,\n        'I-URL_PERSONAL': 11,\n        'O': 12\n    } \n    # Словарь для преобразования меток\n    label_mapping = {\n        'O': 'O',\n        'B-CITY': 'O',\n        'I-CITY': 'O',\n        'B-FIRSTNAME': 'B-NAME_STUDENT',\n        'I-FIRSTNAME': 'I-NAME_STUDENT',\n        'B-USERNAME': 'B-USERNAME',\n        'I-USERNAME': 'B-USERNAME',\n        'B-JOBTYPE': 'O',\n        'B-PREFIX': 'O',\n        'I-PREFIX': 'O',\n        'B-LASTNAME': 'I-NAME_STUDENT',\n        'B-EMAIL': 'B-EMAIL',\n        'I-EMAIL': 'B-EMAIL',\n        'B-NEARBYGPSCOORDINATE': 'O',\n        'I-NEARBYGPSCOORDINATE': 'O',\n        'B-ACCOUNTNUMBER': 'O',\n        'I-ACCOUNTNUMBER': 'O',\n        'B-ACCOUNTNAME': 'O',\n        'I-ACCOUNTNAME': 'O',\n        'B-MIDDLENAME': 'I-NAME_STUDENT',\n        'I-MIDDLENAME': 'I-NAME_STUDENT',\n        'B-COUNTY': 'O',\n        'I-COUNTY': 'O',\n        'B-AGE': 'O',\n        'B-CREDITCARDCVV': 'O',\n        'B-DOB': 'O',\n        'I-DOB': 'O',\n        'B-MASKEDNUMBER': 'O',\n        'I-MASKEDNUMBER': 'O',\n        'B-PASSWORD': 'O',\n        'I-PASSWORD': 'O',\n        'B-SEX': 'O',\n        'B-STATE': 'O',\n        'B-COMPANYNAME': 'O',\n        'I-COMPANYNAME': 'O',\n        'B-PHONEIMEI': 'B-ID_NUM',\n        #'B-PHONEIMEI': 'O',\n        'I-PHONEIMEI': 'I-ID_NUM',\n        #'I-PHONEIMEI': 'O',\n        'B-STREET': 'B-STREET_ADDRESS',\n        'I-STREET': 'I-STREET_ADDRESS',\n        'B-SSN': 'O',\n        'I-SSN': 'O',\n        'B-IPV4': 'O',\n        'I-IPV4': 'O',\n        'B-USERAGENT': 'O',\n        'I-USERAGENT': 'O',\n        'B-MAC': 'O',\n        'I-MAC': 'O',\n        'B-PIN': 'O',\n        'I-PIN': 'O',\n        'B-IP': 'O',\n        'I-IP': 'O',\n        'B-URL': 'B-URL_PERSONAL',\n        'I-URL': 'I-URL_PERSONAL',\n        'B-CURRENCYSYMBOL': 'O',\n        'B-DATE': 'O',\n        'I-DATE': 'O',\n        'B-TIME': 'O',\n        'I-TIME': 'O',\n        'B-VEHICLEVRM': 'O',\n        'I-VEHICLEVRM': 'O',\n        'I-AMOUNT': 'O',\n        'B-ETHEREUMADDRESS': 'O',\n        'I-ETHEREUMADDRESS': 'O',\n        'B-BITCOINADDRESS': 'O',\n        'I-BITCOINADDRESS': 'O',\n        'B-LITECOINADDRESS': 'O',\n        'I-LITECOINADDRESS': 'O',\n        'I-JOBTYPE': 'O',\n        'B-CREDITCARDNUMBER': 'O',\n        'I-CREDITCARDNUMBER': 'O',\n        'B-IPV6': 'O',\n        'I-IPV6': 'O',\n        'I-LASTNAME': 'O',\n        'B-PHONENUMBER': 'B-PHONE_NUM',\n        'I-PHONENUMBER': 'I-PHONE_NUM',\n        'B-CREDITCARDISSUER': 'O',\n        'I-CREDITCARDISSUER': 'O',\n        'B-SECONDARYADDRESS': 'O',\n        'I-SECONDARYADDRESS': 'O',\n        'B-ZIPCODE': 'O',\n        'I-ZIPCODE': 'O',\n        'B-VEHICLEVIN': 'O',\n        'I-VEHICLEVIN': 'O',\n        'I-AGE': 'O',\n        'B-GENDER': 'O',\n        'I-GENDER': 'O',\n        'B-ORDINALDIRECTION': 'O',\n        'B-JOBAREA': 'O',\n        'B-HEIGHT': 'O',\n        'I-HEIGHT': 'O',\n        'B-JOBTITLE': 'O',\n        'I-JOBTITLE': 'O',\n        'B-BUILDINGNUMBER': 'O',\n        'I-BUILDINGNUMBER': 'O',\n        'B-AMOUNT': 'O',\n        'I-STATE': 'O',\n        'I-CURRENCYSYMBOL': 'O',\n        'B-IBAN': 'O',\n        'I-IBAN': 'O',\n        'B-BIC': 'O',\n        'I-BIC': 'O',\n        'B-EYECOLOR': 'O',\n        'B-CURRENCYNAME': 'O',\n        'I-CURRENCYNAME': 'O',\n        'B-CURRENCY': 'O',\n        'I-CURRENCY': 'O',\n        'B-CURRENCYCODE': 'O',\n        'I-CURRENCYCODE': 'O',\n        'I-JOBAREA': 'O',\n        'I-EYECOLOR': 'O',\n        'I-CREDITCARDCVV': 'O'\n    }\n    # Создаем пустой список для конвертации\n    convert_list = [[] for _ in range(13)]\n    # Заполняем список для конвертации меток\n    for idx, (in_label, standart_label) in enumerate(label_mapping.items()):\n        convert_list[standart_label2id[standart_label]].append(idx)\n    return convert_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Конвертирует предсказания из 54 классов в 13 классов\ndef convert_predition(predictions):\n    # Создаем предсказания стандартной размерности.\n    pred_standart = np.zeros((*predictions.shape[:-1], 13))\n    # Конвертируем предсказания из 54 классов в 13 классов\n    convert_list = make_convert_list()\n    predictions = softmax(predictions, axis=-1) \n    for idx, cols in enumerate(convert_list):\n        if cols:\n            # Выбираем срезы по последнему измерению из массива predictions и суммируем их\n            pred_standart[:,:, idx] = np.sum(predictions[:,:, cols], axis=-1)\n    return pred_standart","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Выравнивает второе измерение у двух трехмерных numpy массивов\ndef align_dimension_2(array_1, array_2):\n    max_cols = max(array_1.shape[1], array_2.shape[1])\n            \n    #print('shape', array_1.shape, array_2.shape)\n    # Дополняем массивы нулями до максимальных размерностей\n    array_1 = np.pad(array_1,\n                     ((0, 0),\n                      (0, max_cols - array_1.shape[1]),\n                      (0, 0)),\n                     mode='constant')\n    \n    array_2 = np.pad(array_2,\n                     ((0, 0),\n                      (0, max_cols - array_2.shape[1]),\n                      (0, 0)),\n                     mode='constant')\n\n    return(array_1, array_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Подготавливаем трейнер и все что необходимо для него\ndef prepair_trainer(model_path):\n    global tokenizer, model, collator, args, trainer\n    \n    model = AutoModelForTokenClassification.from_pretrained(model_path)  # Загружаем модель для классификации токенов\n    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)  # Создаем объект для подготовки данных к обучению\n    \n    args = TrainingArguments(  # Устанавливаем аргументы для обучения\n        \".\",\n        per_device_eval_batch_size=1,\n        report_to=\"none\",\n        disable_tqdm=True,\n    )\n\n    trainer = Trainer(  # Создаем объект тренера для управления процессом обучения и оценки\n        model=model,\n        args=args,\n        data_collator=collator,\n        tokenizer=tokenizer,\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Получаем предсказания дробя на батчи\ndef get_batched_predition(model_path):\n    global tokenizer, model, collator, args, trainer\n    #print(p_time(), 'start prepair trainer')\n    prepair_trainer(model_path)\n    #print(p_time(), 'stop prepair trainer')\n\n\n    # Будем делать предсказания по батчам и сохранять предсказания\n    # Индекс в датасете\n    ds_idx = 0\n    # Индекс файла с предсказаниями\n    file_idx = 0\n    while ds_idx < len(ds):\n        # Динамически ищем размер батча, чтобы не слишком маленький. но и не забить память\n        # Размер батча будет тем меньше чем больше токенов\n        end_batch_idx = ds_idx\n        while ((end_batch_idx < len(ds_tokens_len))\n               # Площадь площадь токенов на число документов в предсказании не превышает лимит\n               #and ((ds_tokens_len[end_batch_idx]*ds_tokens_len[end_batch_idx]*ds_tokens_len[end_batch_idx]*(end_batch_idx-ds_idx)) < max_pred_sqaure)\n               and ((end_batch_idx-ds_idx) < max_batch_size)):\n            end_batch_idx += 1\n\n        start_predict = time.time()\n        predictions = trainer.predict(\n            ds.select(range(ds_idx,end_batch_idx))).predictions  # Получаем предсказания модели на нашем наборе данных\n        if is_local:\n            print('start index:', ds_idx,\n                  'tokens_len:', np.round(np.mean(np.array(\n                      ds_tokens_len[ds_idx:end_batch_idx]))).astype(int),\n                  'batch_size:', end_batch_idx-ds_idx,\n                  'batch_time:', round((time.time() - start_predict), 2))\n        if predictions.shape[2] > 13:\n            # Если модель предсказывает больше 13 классов трансформируем предсказания в 13 классов.\n            predictions = convert_predition(predictions)\n        np.savez_compressed(os.path.join(predict_dir, f'preds_{file_idx}.npz'), arr=predictions)\n        file_idx +=1\n        ds_idx = end_batch_idx\n        del predictions  # Удаляем созданные объекты для освобождения памяти\n        torch.cuda.empty_cache()  # Очищаем кэш CUDA, чтобы освободить память GPU\n        gc.collect()  # Вызываем сборщик мусора для очистки неиспользуемой памяти\n        # Ваш код обработки батча\n    del trainer, args, collator, model  # Удаляем созданные объекты для освобождения памяти\n    torch.cuda.empty_cache()  # Очищаем кэш CUDA, чтобы освободить память GPU\n    gc.collect()\n    # Теперь загружаем пресказания, сохраненные ранее на диске\n    for idx in range(file_idx):\n        # print(idx)\n        cur_predictions = np.load(os.path.join(predict_dir, f'preds_{idx}.npz'))['arr']\n        if idx == 0:\n            predictions = cur_predictions\n        else:\n            # Выравниваем массивы\n            # predictions, cur_predictions = align_dimension_2(predictions, cur_predictions)\n\n            max_cols = max(predictions.shape[1], cur_predictions.shape[1])\n                    \n            #print('shape', array_1.shape, array_2.shape)\n            # Дополняем массивы нулями до максимальных размерностей\n            predictions = np.pad(predictions,\n                                 ((0, 0),\n                                  (0, max_cols - predictions.shape[1]),\n                                  (0, 0)),\n                                 mode='constant')\n            \n            cur_predictions = np.pad(cur_predictions,\n                                     ((0, 0),\n                                      (0, max_cols - cur_predictions.shape[1]),\n                                      (0, 0)),\n                                     mode='constant')\n            \n            # Объединяем массивы\n            predictions = np.concatenate((predictions, cur_predictions))\n            del cur_predictions\n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modify_array(arr):\n    arr += 1\n\noriginal_array = np.array([1, 2, 3])\nprint(\"Original array:\", original_array)\n\nmodify_array(original_array)\nprint(\"Array after modification:\", original_array)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Делает предсказание для модели с индексом idx в model_paths\ndef make_predition(idx, model_path, docs_to_process):\n    global weighted_predictions\n    #global raw_model_predictions\n    if is_make_predition or not is_local:\n        # Если указано чделать предсказания на инференсе или если выполняем не локально, а сабми\n        # то в любом случае делаем предсказания и записываем их на диск. Иначе пользуемся теми предсказаниями\n        # что были записаны на диске ранее\n        \n        predictions = get_batched_predition(model_path)\n    \n        #del model, trainer, tokenizer  # Удаляем созданные объекты для освобождения памяти\n        #torch.cuda.empty_cache()  # Очищаем кэш CUDA, чтобы освободить память GPU\n        #gc.collect()\n\n        #if is_local:\n            # Сохраним предсказания модели в список\n            #raw_model_predictions.append(predictions.copy())\n\n        # Конвертирует предсказания из 54 классов в 13 классов\n        #! predictions = convert_predition(predictions)\n        # predictions = convert_predition(predictions)\n        \n        weighted_predictions = softmax(predictions, axis=-1)  # Применяем softmax к предсказаниям и умножаем на вес модели\n\n        if is_local:\n            # Сохраняем взвешенные предсказания на диск\n            np.savez_compressed(os.path.join(intermediate_dir, f'weighted_preds_model_{idx}.npz'), arr=weighted_predictions)\n        \n        # Очищаем память\n        del predictions  # Удаляем созданные объекты для освобождения памяти\n        torch.cuda.empty_cache()  # Очищаем кэш CUDA, чтобы освободить память GPU\n        gc.collect()  # Вызываем сборщик мусора для очистки неиспользуемой памяти\n    elif is_local:\n        # Загружаем взвешенные предсказания модел\n        weighted_predictions = np.load(os.path.join(intermediate_dir, f'weighted_preds_model_{idx}.npz'))['arr']\n        weighted_predictions = get_pred_part(weighted_predictions, docs_to_process)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Делает суммарное предсказание, на указанных документах,\n# с указанным укзанием по времени предсказания\n# start_idx - индекс модели в model_paths с которой начинать предсказание\n# docs_to_process - список в котором указано какие документы обрабатывать\n# time_limit - порог времения выполнения ноубтука, которое нельзя превысить\n# sum_predictions_name - наименование файла в который записывать суммарные предсказания\n# model_time_mul при вычислениии остатка времени максимальное время модели домножается на это число\n\ndef make_sum_predition(start_idx,\n                       docs_to_process,\n                       time_limit,\n                       sum_predictions_name,\n                       model_time_mul):\n#def make_sum_predition(start_idx, time_limit, sum_predictions_name):\n    global weighted_predictions, model_predictions\n    #global raw_model_predictions\n    \n    max_model_time = 0 # Время которое тренировалась модель\n    grp = []\n    model_weights = 0  # Суммарный вес моделей в группе\n    # Список с прекдсказаниями моделей\n    if is_local:\n        model_predictions = []\n        #raw_model_predictions = []\n    for idx, (model_path, weight) in enumerate(model_paths.items()):  # Итерируемся по путям и весам моделей\n    \n        if idx < start_idx:\n            continue\n        start_model_time = time.time()\n        print(p_time(), f'start {model_path}')\n        \n        make_predition(idx, model_path, docs_to_process)\n    \n         #Добавляем оттренированную модель к индексу\n        grp.append(idx)\n    \n        if is_local:\n            # Сохраним предсказания модели в список\n            model_predictions.append(weighted_predictions.copy())\n    \n        weighted_predictions *= weights[idx]\n        model_weights += weights[idx]  # Добавляем вес текущей модели к общему весу моделей в группе\n        \n        if idx > start_idx:\n            #Загружаем накопленные предсказания, если уже не первый цикл\n            sum_predictions = np.load(os.path.join(intermediate_dir, f'{sum_predictions_name}.npz'))['arr']\n            #sum_predictions, weighted_predictions = align_dimension_2(sum_predictions, weighted_predictions)\n            \n            max_cols = max(sum_predictions.shape[1], weighted_predictions.shape[1])\n            # Дополняем массивы нулями до максимальных размерностей\n            sum_predictions = np.pad(sum_predictions,\n                                     ((0, 0),\n                                      (0, max_cols - sum_predictions.shape[1]),\n                                      (0, 0)),\n                                     mode='constant')\n            \n            weighted_predictions = np.pad(weighted_predictions,\n                                          ((0, 0),\n                                          (0, max_cols - weighted_predictions.shape[1]),\n                                          (0, 0)),\n                                         mode='constant')\n            \n        # Накапливаем предсказания, суммируя их\n        sum_predictions = (weighted_predictions if idx == start_idx\n                                        else sum_predictions + weighted_predictions) \n        # Сохраняем агрегированные предсказания на диск\n        np.savez_compressed(os.path.join(intermediate_dir, f'{sum_predictions_name}.npz'), arr=sum_predictions)\n        del weighted_predictions, sum_predictions\n        \n        cur_time = time.time()\n        print('model time:', cur_time - start_model_time)\n        max_model_time = max(max_model_time, cur_time - start_model_time)\n        # Смотрим, достаточно ли времени для тренировки еще одной модели и чтобы остался после этого еще час\n        if ((cur_time - notebook_starttime) > (time_limit - (max_model_time*model_time_mul))):\n        #if ((cur_time - notebook_starttime) > (40*2 - max_model_time)):\n            print('Превышено время для тренеровок моделей')\n            print('ноутбук выполнялся:', (cur_time - notebook_starttime))\n            print('max_model_time:', max_model_time)\n            print('Оттренированы модели:', grp)\n            break\n    \n    #weighted_average_predictions = np.load(os.path.join(intermediate_dir, f'{sum_predictions_name}.npz'))['arr']\n    #weighted_average_predictions /= model_weights\n    return model_weights, idx, max_model_time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Возвращает предсказания с установленным порогом threshold\ndef get_preds_final(threshold):\n    global weighted_average_predictions\n    \n    preds = weighted_average_predictions.argmax(-1)  # Получаем предсказания, выбирая максимальное значение по последнему измерению\n\n    preds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)  # Получаем предсказания без учета класса 'O', выбирая максимумы до 12-го индекса\n    \n    O_preds = weighted_average_predictions[:,:,12]  # Извлекаем предсказания для класса 'O' (12-й столбец)\n    \n    return np.where(O_preds < threshold, preds_without_O, preds), O_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Возвращает предсказания с индивидуальным для каждого класса threshold","metadata":{}},{"cell_type":"code","source":"# Возвращает предсказания с индивидуальным для каждого класса threshold,\ndef get_preds_final_unique():\n    global weighted_average_predictions\n    #global thresholds_expanded, mask, weighted_average_predictions_copy, preds, preds_without_O, preds_without_O_max\n    # Повторяем массив порогов по размерностям массива данных\n    #weighted_average_predictions_copy = weighted_average_predictions.copy()\n    for idx in range(weighted_average_predictions.shape[0]):\n        thresholds_expanded = np.tile(\n            np.array(thresholds_unique),\n            (weighted_average_predictions.shape[1], 1))\n        \n        # Создаем маску, где True соответствует элементам массива, которые меньше соответствующих порогов\n        mask = weighted_average_predictions[idx] < thresholds_expanded\n        \n        # Зануляем элементы массива, удовлетворяющие маске\n        weighted_average_predictions[idx][mask] = 0\n        del mask, thresholds_expanded\n    \n    preds = weighted_average_predictions.argmax(-1)\n    preds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\n    preds_without_O_max = weighted_average_predictions[:,:,:12].max(-1)\n    O_preds = weighted_average_predictions[:,:,12]  # Извлекаем предсказания для класса 'O' (12-й столбец)\n    \n    return np.where(preds_without_O_max > 0, preds_without_O, preds), O_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weighted_average_predictions[0,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weighted_average_predictions_copy[0,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mask[0,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Возвращает список где единицами указаны документы в которых есть \n# некоторые сомнения есть ли перс информация или нет. \ndef get_docs_with_doubts():\n    global weighted_average_predictions\n    \n    name_count = 0\n    no_name_count = 0\n    O_preds = weighted_average_predictions[:,:,12]\n    # В этом списке 1 будет в документе где есть персональная информация,\n    # а ноль, если нет персоанльной информации\n    docs_with_doubts = [0] * len(doc_list)\n    for i, input_ids in enumerate(ds['input_ids']):\n        # Смотрим нашлим ли хоть одино не 'O' токен\n        if np.any((O_preds[i][0:len(input_ids)] < find_pii_docs_threshold) & (O_preds[i][0:len(input_ids)] > .5)):\n            # Если в документе есть PII токены.\n            name_count += 1\n            docs_with_doubts[i] = 1\n        else:\n            # Если в документе только токены O найдены\n            no_name_count += 1\n    \n    print('doc without doubts:', no_name_count, '; doc with doubts:', name_count)\n    return(docs_with_doubts)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Создаем случайный трехмерный массив для примера\narray = np.random.randint(0, 100, size=(5, 5, 13))\n\n# Список порогов\n#thresholds = [10, 20, 99, 40, 96, 60, 70, 80, 90, 10, 20, 30, 40]\nthresholds = [96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 0]\n\n# Повторяем массив порогов по размерностям массива данных\nthresholds_expanded = np.tile(np.array(thresholds), (array.shape[0], array.shape[1], 1))\n\n# Создаем маску, где True соответствует элементам массива, которые меньше соответствующих порогов\nmask = array < thresholds_expanded\n\n# Зануляем элементы массива, удовлетворяющие маске\nweighted_average_predictions = array.copy()\nweighted_average_predictions[mask] = 0\n\npreds = weighted_average_predictions.argmax(-1)\npreds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\npreds_without_O_max = weighted_average_predictions[:,:,:12].max(-1)\nret = np.where(preds_without_O_max > 0, preds_without_O, preds)\n#print(array1)\n'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Возвращает список где единицами указаны документы в которых есть персональная информация\ndef get_docs_with_pii(preds_final):\n    name_count = 0\n    no_name_count = 0\n    # В этом списке 1 будет в документе где есть персональная информация,\n    # а ноль, если нет персоанльной информации\n    docs_with_pii = [0] * len(doc_list)\n    for i, input_ids in enumerate(ds['input_ids']):\n        # Смотрим нашлим ли хоть одино не 'O' токен\n        if np.any(preds_final[i][0:len(input_ids)] != 12):\n            # Если в документе есть PII токены.\n            name_count += 1\n            docs_with_pii[i] = 1\n        else:\n            # Если в документе только токены O найдены\n            no_name_count += 1\n    \n    print('doc without names:', no_name_count, '; doc with names:', name_count)\n    return(docs_with_pii)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Возвращает список где единицами указаны документы которые попали в указанный диапазон\ndef get_docs_in_range(range_start, range_end):\n    docs_in_range = [0] * len(doc_list)\n    docs_in_range[range_start:range_end] = [1] * (range_end - range_start)\n    \n    return(docs_in_range)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Собираем \ndef make_average_predictions():\n    global weighted_average_predictions\n    # Добавим к предсказаниям на всем датасете, предсказания дополнительных моделей только на текстах с pii\n    weighted_average_predictions = np.load(os.path.join(intermediate_dir, f'all_sum_predictions.npz'))['arr']\n    if has_short_doc:\n        short_sum_predictions = np.load(os.path.join(intermediate_dir, f'short_sum_predictions.npz'))['arr']\n        # Выравниваем вторые размерности моссивов (число токенов в документах\n        weighted_average_predictions, short_sum_predictions = align_dimension_2(weighted_average_predictions, short_sum_predictions)\n    '''\n    if has_pii_doc:\n        pii_sum_predictions = np.load(os.path.join(intermediate_dir, f'pii_sum_predictions.npz'))['arr']\n        # Выравниваем вторые размерности моссивов (число токенов в документах\n        weighted_average_predictions, pii_sum_predictions = align_dimension_2(weighted_average_predictions, pii_sum_predictions)\n\n    if has_doubts_doc:\n        doubts_sum_predictions = np.load(os.path.join(intermediate_dir, f'doubts_sum_predictions.npz'))['arr']\n        weighted_average_predictions, doubts_sum_predictions = align_dimension_2(weighted_average_predictions, doubts_sum_predictions)\n    '''\n\n    #pii_idx = 0\n    #doubts_idx = 0\n    short_idx = 0\n    for weighted_idx in range(weighted_average_predictions.shape[0]):\n        cur_model_weights = all_model_weights\n        if has_short_doc:\n            if docs_with_short[short_idx] == 1:\n                #Есди в документе есть персональныя информация\n                cur_model_weights += short_model_weights\n                weighted_average_predictions[weighted_idx] += short_sum_predictions[short_idx]\n                short_idx += 1\n        '''\n        if has_pii_doc:\n            if docs_with_pii[weighted_idx] == 1:\n                #Есди в документе есть персональныя информация\n                cur_model_weights += pii_model_weights\n                weighted_average_predictions[weighted_idx] += pii_sum_predictions[pii_idx]\n                pii_idx += 1\n        if has_doubts_doc:\n            if docs_with_doubts[weighted_idx] == 1:\n                #Есди в документе есть токены с сомнениями\n                cur_model_weights += doubts_model_weights\n                weighted_average_predictions[weighted_idx] += doubts_sum_predictions[doubts_idx]\n                doubts_idx += 1\n        '''  \n        weighted_average_predictions[weighted_idx] /= cur_model_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Главный цикл предсказаний","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Отмечаем что в начале обрабатываем все документы (едеиницы в каждом элементе списка всех документов) \nprint(p_time(), 'Тренируем модели на всем датасете')\nall_docs = [1] * len(doc_list)\nmake_dataset(all_docs)\nall_model_weights, last_idx, max_model_time = make_sum_predition(\n    start_idx=0,\n    docs_to_process=all_docs,\n    time_limit=all_docs_time_limit,\n    sum_predictions_name='all_sum_predictions',\n    model_time_mul=2\n)\nprint ('all_model_weights:',all_model_weights,'last_idx:',last_idx)\nweighted_average_predictions = np.load(os.path.join(intermediate_dir, f'all_sum_predictions.npz'))['arr']\nweighted_average_predictions /= all_model_weights\n\n# Поcтавится в True, если были предсказания моделей только по документам\n# с токенами по которым были сомнения\n#has_doubts_doc = False\n# Поcтавится в True, если были предсказания моделей только по документам\n# с уже найденными токенами pii\n#has_pii_doc = False\n# Поcтавится в True, если короткие документы\nhas_short_doc = False\n\n# ***************************************************************\n# Теперь дополнительно получаем предсказания с новыми моделя уже только в документах,\n# где ранее нашли какоую то персональную информацию. Потому что таких документов меньше чем всех\n# и можно будет прогнать их по большему числу и, возможно, более тяжелых моделей\n# ***************************************************************\n#! preds_pii = get_preds_final(threshold=find_pii_docs_threshold)\n# Получаем список в котором единицами указаны документы с персоанльной информацией\n# Получаем финальные прдесказания\n#! docs_with_pii = get_docs_with_pii(preds_pii)\n#! del preds_pii\n\n#docs_with_pii = get_docs_in_range(0, round(len(doc_list)*range_share))\n#docs_with_doubts = get_docs_with_doubts()\n\n# Доля докуметнтов с токенами pii\n#! share = sum(docs_with_pii) / len(docs_with_pii)\n# estimate_model_time = max_model_time * share\n\ndocs_with_short = get_docs_in_range(0, round(len(doc_list)*range_share))\nestimate_model_time = max_model_time * range_share\nprint('estimate_model_time:',estimate_model_time)\ncur_time = time.time()\nprint(p_time())\n# Проверяем остались ли еще модели для предсказания и есть ли еще время\nif ((last_idx + 1 < len(model_paths))\n    and ((cur_time - notebook_starttime) < (short_docs_time_limit - estimate_model_time))):\n#if 1 == 2:\n    # Если еще остались модели для тренировки\n    print(p_time(), 'Тренируем модели только на коротких документах')\n    has_short_doc = True\n    del weighted_average_predictions\n    # Делаем датасет только для коротких документов\n    make_dataset(docs_with_short)\n    short_model_weights, last_idx, max_model_time = make_sum_predition(\n        start_idx=(last_idx + 1),\n        docs_to_process=docs_with_short,\n        time_limit=short_docs_time_limit,\n        sum_predictions_name='short_sum_predictions',\n        model_time_mul=1\n    )\n    del ds\n    make_average_predictions()\n    \n#if has_pii_doc or has_doubts_doc:\nif has_short_doc:\n    \n    #Если переделывали датасет - возвращаем его на место\n    #Сохраняем и удаляем веса, чтобы высвободить память перед ресурсоемкой опперацией\n    np.savez_compressed(os.path.join(intermediate_dir, f'all_sum_predictions.npz'), arr=weighted_average_predictions)\n    del weighted_average_predictions\n    # Возвращаем датасет со всеми документами на место\n    make_dataset(all_docs)\n    weighted_average_predictions = np.load(os.path.join(intermediate_dir, f'all_sum_predictions.npz'))['arr']\n# ***************************************************************\n# Получаем финальные прдесказания\n# ***************************************************************\n# Предсказание на основе общего порога для не персональной информации\n# preds_final, O_preds = get_preds_final(threshold=final_docs_threshold)\n\n# Предсказание на основе раздельных порогов для каждого типа метки\npreds_final, O_preds = get_preds_final_unique()\n#docs_with_pii = get_docs_with_pii(preds_final)\ndel weighted_average_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Убранные доопредсказяния доп. моделями","metadata":{}},{"cell_type":"code","source":"# Пока убрал. Это доработка разных документов, которые не очень сработали.\n'''\n# ***************************************************************\n# Теперь дополнительно получаем предсказания с новыми моделя уже только в документах,\n# где ранее нашли какоую то персональную информацию. Потому что таких документов меньше чем всех\n# и можно будет прогнать их по большему числу и, возможно, более тяжелых моделей\n# ***************************************************************\n#! preds_pii = get_preds_final(threshold=find_pii_docs_threshold)\n# Получаем список в котором единицами указаны документы с персоанльной информацией\n# Получаем финальные прдесказания\n#! docs_with_pii = get_docs_with_pii(preds_pii)\n#! del preds_pii\n\ndocs_with_pii = get_docs_in_range(0, round(len(doc_list)*range_share))\ndocs_with_doubts = get_docs_with_doubts()\n# Доля докуметнтов с токенами pii\n#! share = sum(docs_with_pii) / len(docs_with_pii)\n#1 estimate_model_time = max_model_time * share\nestimate_model_time = max_model_time * range_share\nprint('estimate_model_time:',estimate_model_time)\ncur_time = time.time()\nprint(p_time())\n# Проверяем остались ли еще модели для предсказания и есть ли еще время\nif ((last_idx + 1 < len(model_paths))\n    and ((cur_time - notebook_starttime) < (pii_docs_time_limit - estimate_model_time))):\n#if 1 == 2:\n    # Если еще остались модели для тренировки\n    print(p_time(), 'Тренируем модели только на документах с pii')\n    has_pii_doc = True\n    del weighted_average_predictions\n    # Делаем датасет только для документов где нашли персональную информацию\n    make_dataset(docs_with_pii)\n    pii_model_weights, last_idx, max_model_time = make_sum_predition(\n        start_idx=(last_idx + 1),\n        docs_to_process=docs_with_pii,\n        time_limit=pii_docs_time_limit,\n        sum_predictions_name='pii_sum_predictions',\n        model_time_mul=1\n    )\n    del ds\n    make_average_predictions()\n\n# ***************************************************************\n# Теперь дополнительно получаем предсказания с новыми моделя уже только в документах,\n# где ранее нашли какоую то персональную информацию. Потому что таких документов меньше чем всех\n# и можно будет прогнать их по большему числу и, возможно, более тяжелых моделей\n# ***************************************************************\n# Доля докуметнтов с сомнениями\nshare = sum(docs_with_doubts) / len(docs_with_doubts)\nestimate_model_time = max_model_time * share\nprint('estimate_model_time:',estimate_model_time)\ncur_time = time.time()\nprint(p_time())\n# Проверяем остались ли еще модели для предсказания и есть ли еще время\nif ((last_idx + 1 < len(model_paths))\n    and ((cur_time - notebook_starttime) < (doubts_docs_time_limit - estimate_model_time))):\n    print(p_time(), 'Тренируем модели только на документах в которых есть токены с сомнениями')\n    has_doubts_doc = True\n\n    del weighted_average_predictions\n    # Делаем датасет только для документов где нашли персональную информацию\n    make_dataset(docs_with_doubts)\n    doubts_model_weights, last_idx, max_model_time = make_sum_predition(\n        start_idx=(last_idx + 1),\n        docs_to_process=docs_with_doubts,\n        time_limit=doubts_docs_time_limit,\n        sum_predictions_name='doubts_sum_predictions',\n        model_time_mul=1\n    )\n    del ds\n    make_average_predictions()\n'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Перенос предсказаний на исходную токенизацию","metadata":{}},{"cell_type":"code","source":"%%time\ntriplets = []  # Инициализируем список для троек (в данном случае не используется)\npairs = set()  # Используем множество для пар, чтобы проверка наличия элемента была быстрее O(1), чем в списке O(n)\n\nprocessed = []  # Список для обработанных записей\n\n# Для каждого прогноза, соответствия токенов, смещений, токенов и документа в наборе данных\n#for p, token_map, offsets, tokens, doc, doc_with_pii in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], docs_with_pii):\nfor p, O_p, token_map, offsets, tokens, doc in zip(preds_final, O_preds, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    #if doc_with_pii == 0:\n        # Если в документе нет перс\n            #continue\n    token_pred_ind = 0 # Индекс токена предсказания\n\n    if is_local:\n        index_to_change = df_result.index[df_result['document'] == doc][0]\n    # Итерируемся по каждому прогнозу токена и соответствующим смещениям\n    for token_pred, O_pred, (start_idx, end_idx) in zip(p, O_p, offsets):\n        token_pred_ind += 1\n        label_pred = id2label[str(token_pred)]  # Получаем предсказанную метку для токена\n\n        # Если сумма начального и конечного индекса равна нулю, переходим к следующей итерации\n        if start_idx + end_idx == 0:\n            continue\n\n        # Если соответствие токена при начальном индексе равно -1, увеличиваем начальный индекс\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # Игнорируем начальные пробельные токены (\"\\n\\n\")\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        # Если начальный индекс превышает длину соответствия токенов, прерываем цикл\n        if start_idx >= len(token_map):\n            break\n\n        token_id = token_map[start_idx]  # Получаем ID токена при начальном индексе\n                \n        \"\"\"\n            {'B-EMAIL',\n             'B-ID_NUM',\n             'B-NAME_STUDENT',\n             'B-PHONE_NUM',\n             'B-STREET_ADDRESS',\n             'B-URL_PERSONAL',\n             'B-USERNAME',\n             'I-ID_NUM',\n             'I-NAME_STUDENT',\n             'I-PHONE_NUM',\n             'I-STREET_ADDRESS',\n             'I-URL_PERSONAL',\n             'O'}\n        \"\"\"\n\n        if is_local:\n            # Устанавливаем связь между индексами предсказаний и индексами токенов в тесте на старом токенизаторе\n            df_result.loc[index_to_change, 'pred_ind'][token_id] = token_pred_ind - 1\n        \n        # Игнорируем предсказания \"O\" и токены пробелов, а также специфичные метки, например \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\"\n        #if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n        if label_pred in (\"O\") or token_id == -1:\n            continue\n\n        pair = (doc, token_id)  # Формируем пару из документа и ID токена\n\n        # Если такой пары нет в множестве, добавляем запись в список обработанных\n        if pair not in pairs:\n            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id], \"o_pred\": O_pred})\n            pairs.add(pair)  # Добавляем пару в множество, чтобы избежать повторений\n","metadata":{"papermill":{"duration":0.111518,"end_time":"2024-03-16T07:35:29.39561","exception":false,"start_time":"2024-03-16T07:35:29.284092","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Нахождение токенов и телефонов регулярными выражениями","metadata":{}},{"cell_type":"code","source":"from spacy.lang.en import English\nnlp = English()\n\ndef find_span(target: list[str], document: list[str]) -> list[list[int]]:\n    idx = 0\n    spans = []\n    span = []\n\n    for i, token in enumerate(document):\n        if token != target[idx]:\n            idx = 0\n            span = []\n            continue\n        span.append(i)\n        idx += 1\n        if idx == len(target):\n            spans.append(span)\n            span = []\n            idx = 0\n            continue\n    \n    return spans","metadata":{"_cell_guid":"fe4b7ae3-0a11-4266-bc31-840518f770b6","_uuid":"533dca77-5c27-4f55-ae6a-ab1195fd4ad0","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":3.866808,"end_time":"2024-03-16T07:35:33.270921","exception":false,"start_time":"2024-03-16T07:35:29.404113","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Компиляция регулярных выражений для поиска PII\nemail_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\nphone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n\n# Инициализация списков для хранения найденных данных\nemails = []\nphone_nums = []\n\n# Функция для поиска индексов токенов, соответствующих целевой последовательности\ndef find_span(target, document):\n    spans = []\n    target_len = len(target)\n    for i in range(len(document) - target_len + 1):\n        if document[i:i + target_len] == target:\n            spans.append(range(i, i + target_len))\n    return spans\n\n#!\n# Обработка каждого документа в данных\n#for _data in data:\nfor _data in test_data:\n    try:\n        # Поиск и добавление адресов электронной почты\n        for token_idx, token in enumerate(_data[\"tokens\"]):\n            if email_regex.fullmatch(token):\n                emails.append(\n                    {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token, \"o_pred\": 1}\n                )\n\n        # Поиск телефонных номеров в полном тексте документа\n        matches = phone_num_regex.findall(_data[\"full_text\"])\n        for match in matches:\n            target = [t.text for t in nlp.tokenizer(match)]\n            matched_spans = find_span(target, _data[\"tokens\"])\n\n            # Добавление информации о телефонных номерах\n            for matched_span in matched_spans:\n                for intermediate, token_idx in enumerate(matched_span):\n                    prefix = \"I\" if intermediate else \"B\"\n                    phone_nums.append(\n                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx], \"o_pred\": 1}\n                    )\n    except Exception as e:\n        print(f\"Ошибка при обработке документа {_data['document']}: {e}\")\n\n# Вывод результатов для демонстрации\nprint(f\"Найдено адресов электронной почты: {len(emails)}\")\nprint(f\"Найдено телефонных номеров: {len(phone_nums)}\")\n\npass","metadata":{"papermill":{"duration":0.041014,"end_time":"2024-03-16T07:35:33.365526","exception":false,"start_time":"2024-03-16T07:35:33.324512","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Построцессинг\nОбрабатываем найденные персональные токены на предыдущих этапах их поиска нейронными сетями и шаблонами","metadata":{}},{"cell_type":"markdown","source":"### Добавление токенов с адресами","metadata":{}},{"cell_type":"code","source":"%%time\n# Пометим пропущенные токены в песональных адресах\n# Дополнительные токены для адресов\nstreets_add = []\nprev_label = ''\nprev_token = ''\nfor row in processed:\n    if row['label'] == 'I-STREET_ADDRESS' and prev_label == 'I-STREET_ADDRESS':\n        # Если предыдущий и последующий токены отмечены как I-STREET_ADDRESS то помечаем токен между ними тем же.\n        if row['token'] == prev_token + 2:\n            # Если между токенами помечеными I-STREET_ADDRESS есть неразмеченный токен то добавим его\n            doc_index = doc_list.index(row['document'])\n            # Строка токена которую добавляем\n            #token_str_add = ds['tokens'][doc_index][row['token'] - 1]\n            token_str_add = test_data[doc_index]['tokens'][row['token'] - 1]\n            print(\"Добавляем токен:\",row['document'], row['token'], f'[{token_str_add}]')\n            streets_add.append({\"document\": row['document'],\n                                \"token\": row['token'] - 1,\n                                \"label\": 'I-STREET_ADDRESS',\n                                \"token_str\": token_str_add,\n                                \"o_pred\": 1})\n    prev_label = row['label']\n    prev_token = row['token']","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Создаем предварительный датафрейм для анализа\n#df_pred = pd.DataFrame(processed + streets_add + phone_nums + emails)\ndf_pred = pd.DataFrame(processed + streets_add)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Чистка токенов\nУбираем из найденных персональных токенов, которые не соответсвуют некоторой заранее заданной логике.\nСкажем слишком короткие, не соответсвуют шаблону, идут в неправильном порядке или что-то подобное","metadata":{}},{"cell_type":"code","source":"processed_copy = processed.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#processed = processed_copy.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ni = 0\ndelete_count = 0 # Число удаленных\nwhile i < len(processed):\n    is_del_el = False # Нужно ли будет удалять из помеченных найденный токен\n    # Номер документа\n    doc_idx = processed[i]['document']\n    # Число токенов в документе\n    tokens_in_doc_count = (df_pred['document'] == doc_idx).sum()\n    \n    # Чистка начальных токенов за которыми не следуют токены продолжения\n    if bool(re.search(r'^B-', processed[i]['label'])):\n        # Смотрим есть ли продолжение у персональной ифонмации\n        is_next_token = False\n        if i+1 < len(processed):\n            if bool(re.search(r'^I-', processed[i+1]['label'])):\n                is_next_token = True\n        if not(is_next_token):\n            # Усли есть начальный токен и нет у него токена продолжения,\n            # то дальше смотрим насколько он удовлетворяет минимальным условиям\n\n            # B-ID_NUM\n            if (processed[i]['label']  == 'B-ID_NUM'):\n                if not(bool(re.search(r'\\d{2}', processed[i]['token_str']))):\n                    #Нет ни двух цифр в токене и следующий токен не I-ID_NUM, тогда улаляем\n                    is_del_el = True\n                if len(processed[i]['token_str']) <=3:\n                    is_del_el = True\n            \n            # B-STREET_ADDRESS\n            if (processed[i]['label']  == 'B-STREET_ADDRESS'):\n                # Удаляем адрес короче 10 символов\n                if len(processed[i]['token_str']) <=10:\n                    is_del_el = True\n\n            # B-NAME_STUDENT\n            if (processed[i]['label']  == 'B-NAME_STUDENT'):\n                if len(processed[i]['token_str']) <=1:\n                    is_del_el = True\n            \n            # B-PHONE_NUM\n            if (processed[i]['label']  == 'B-PHONE_NUM'):\n                if len(processed[i]['token_str']) <=4:\n                    is_del_el = True\n            \n            # B-PHONE_NUM\n            if (processed[i]['label']  == 'B-EMAIL'):\n                if not(bool(re.search(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', processed[i]['token_str']))):\n                    is_del_el = True\n\n    # Чистка имен B-NAME_STUDENT не соответствующих шаблону\n    if not(is_del_el) and (processed[i]['label']  == 'B-NAME_STUDENT'):\n        if not(bool(re.search(r'^[A-Z][a-z]+$', processed[i]['token_str']))):\n            # Если имя не соответствует шалблону (Большая буква потом строчные) то удаляем\n            is_del_el = True\n        # Проверяем, существует ли в датафрейме хотя бы одна строка без метки 'I-NAME_STUDENT' и с номером документа\n        if not any((df_pred['document'] == doc_idx) & (df_pred['label'] == 'I-NAME_STUDENT')):\n            #Если нет ни одного токена с I-NAME_STUDENT\n            name_count = df_pred[df_pred['document'] == doc_idx]['token_str'].value_counts().get(processed[i]['token_str'], 0)\n            # print(doc_idx, processed[i]['token_str'], name_count)\n            #if tokens_in_doc_count == name_count and name_count >= 3:\n            # Если превышено минимальной число одинаковых имен min_name_count, то удаляем\n            min_name_count = 1\n            if name_count >= min_name_count:\n                min_value = df_pred[(df_pred['document'] == doc_idx) & (df_pred['token_str'] == processed[i]['token_str'])]['o_pred'].min()\n                #if processed[i]['o_pred'] > 0.2:\n                if min_value > 0.4:\n                    # Если вероятность что это было обычный токен достаточно высока\n                    #is_del_el = True\n                    pass\n    \n    # Чистка имен I-NAME_STUDENT не соответствующих шаблону\n    if not(is_del_el) and (processed[i]['label']  == 'I-NAME_STUDENT'):\n        if not(bool(re.search(r'^[A-Z][a-z\\.]+$', processed[i]['token_str']))):\n            # Если имя не соответствует шалблону (Большая буква потом строчные) то удаляем\n            is_del_el = True\n\n        # Чистка I-NAME_STUDENT без началльного токена\n        if i > 0:\n            if not(((processed[i-1]['label'] == 'I-NAME_STUDENT')\n                   or (processed[i-1]['label'] == 'B-NAME_STUDENT'))\n                  and ((processed[i]['token'] - processed[i-1]['token']) < 7)):\n                # Если продолжение имени идет без начала и чтобы не далеко, не дальше 7 докенов, то удаляем его\n                is_del_el = True\n\n    # Чистка слишком коротких B-URL_PERSONAL\n    if not(is_del_el) and (processed[i]['label']  == 'B-URL_PERSONAL'):\n        if len(processed[i]['token_str']) <=1:\n            # Если слишком короткая url - удаляем\n            is_del_el = True\n\n        if bool(re.search(r'(youtu\\.be|youtube\\.com)', processed[i]['token_str'])):\n            # Если ютуб\n            if tokens_in_doc_count == 1:\n                # Если кроме ютуба ничего нет\n                if processed[i]['o_pred'] > 0.2:\n                    # Если вероятность что это было обычный токен достаточно высока\n                    #is_del_el = True\n                    pass\n            \n    if is_del_el:\n        print('удаляем:',processed[i]['label'],processed[i]['token_str'],'doc:', doc_idx, 'o_pred:', processed[i]['o_pred'])\n        del processed[i]\n        delete_count += 1\n    else:\n        i += 1\nprint('Всего удалили', delete_count)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Удаляем слишком длинные списки имен","metadata":{}},{"cell_type":"code","source":"# Удяляем слишком длинные списки имен которые модель приняла за персональные данные,\n# Но это может быть список литературы или что-то подобное\n\ndef del_names_list(processed):\n    tokens_to_del = [] # список индексов токенов в списке processed, которые нужно удалить\n    candidates = [] # список кандидатов на добавление в список для удаления\n    candidates_names = [] # список кандидатов на добавление в список для удаления\n    b_name_count = -1 # Число именно начальных токенов в последовательности имен\n    last_doc = -1 # Последний обработанный документ\n    last_token = -1 # Последний обработанный токен\n    for i in range(len(processed)):\n        if ((processed[i]['label'] == 'I-NAME_STUDENT')\n            or (processed[i]['label'] == 'B-NAME_STUDENT')):\n               \n            # Если список имен уже начался проверяем насколько далеко \n            # отстоит этот токен от предыдущего токена с именем.\n            # Нужно чтобы отстоял не более чем на два токена\n            token_diff = 5 # Допустимое число токенов между именами, чтобы это была еще последовательность\n            if  ((processed[i]['document'] == last_doc)\n                 and ((last_token + token_diff) >= processed[i]['token'])):\n                if (processed[i]['label'] == 'B-NAME_STUDENT'):\n                    b_name_count += 1\n            else:\n                # Список имен прекратился\n                if b_name_count > 2:\n                    # Нашли именно несколько подряд токенов с именами, добавляем их в список для удаления\n                    is_duplicates = False\n                    # Но сначала посмотрим нет ли повторяющихся имен в списке\n                    for name in candidates_names:\n                        if candidates_names.count(name) > 1:\n                            is_duplicates = True\n                    if not (is_duplicates):\n                        tokens_to_del.extend(candidates)\n                candidates = []\n                candidates_names = []\n                if (processed[i]['label'] == 'B-NAME_STUDENT'):\n                    b_name_count = 1\n\n            \n            #Действия необходимые произвести при начале или продолжении списка имен\n            last_doc = processed[i]['document']\n            last_token = processed[i]['token']\n            candidates.append(i)\n            candidates_names.append(processed[i]['token_str'])\n    return(tokens_to_del)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Создаем список для удаления лишних токенов с именами, которых много подряд\ntokens_to_del = del_names_list(processed)\n# Удаляем токены с именами подряд из помеченных как персональная информация\nprocessed = [el for i, el in enumerate(processed) if i not in tokens_to_del]\nprint('Удалено помеченнных как pii токенов с именами идущих подряд:', len(tokens_to_del))\n'''\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processed = processed_copy.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Сохранение сабмита","metadata":{}},{"cell_type":"code","source":"#df = pd.DataFrame(processed + streets_add + phone_nums + emails)\ndf = pd.DataFrame(processed + streets_add)\n#df = pd.DataFrame(processed + streets_add)\n\n# Assign each row a unique 'row_id'\ndf[\"row_id\"] = list(range(len(df)))\n\n# Display a glimpse of the first 100 rows of your data\ndisplay(df.head(100))\n\n# Cast your findings into a CSV file for further exploration\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{"_cell_guid":"05ddc5d2-3847-4748-b261-2e18ff1fa6bf","_uuid":"8dab8773-a604-4cfc-b0eb-955043f2c487","papermill":{"duration":0.035435,"end_time":"2024-03-16T07:35:33.409342","exception":false,"start_time":"2024-03-16T07:35:33.373907","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Вывод результатов\nВыводим те токены с персональной информацнией, что не удалось найти или что нашли лишнее с подробным описанием.","metadata":{}},{"cell_type":"code","source":"# Выводет токены вокруг указанного чтобы посмотреть контекст\ndef print_fragment():\n    if is_print_details:\n        print('Фрагмент:')\n    fragment_len = 100 # Устанавливаем что будем выводить токены 7 до и 7 после\n    token_arr = []\n    label_arr = []\n    for i in range(max(0, token_id - fragment_len),\n                   min(len(tokens) - 1, token_id + fragment_len)):\n    #for i, token in enumerate(tokens[token_id - fragment_len:token_id + fragment_len]):\n        token = tokens[i]\n        token = token.replace('\\n', '\\\\n')\n        if i == token_id:\n            # Помечаем зеленым цветом если нашли лишеюю персональную информацию\n            # Помечаем красным цветом если если не персональную информацию\n            color = ''\n            if true_label == \"O\":\n                # Если истинная метка токена была что это не персональная ифномация\n                color = Fore.GREEN\n                metrics['FP'] += 1\n            elif pred_label == \"O\":\n                # Если истинная метка токена была что это персональная ифномация,\n                # но модель определила как не персональныю информацию\n                color = Fore.RED\n                metrics['FN'] += 1\n            else:\n                # И истинная метка было что персональня инфа и модель так определила что персональня инфа\n                # Но модель не правильно определила тип персональной инфф для токена\n                metrics['err_dist'] += 1\n                \n            color = Fore.GREEN if true_label == \"O\" else (Fore.RED if pred_label == \"O\" else '')\n            excl = '' if true_label == \"O\" else ('!' if pred_label == \"O\" else '?')\n                \n            token_str = f'***{color}{excl}{token}{Style.RESET_ALL}***'\n        else:\n            token_str = f\"{token}\"\n        token_arr.append(f'{token_str} ')\n        label_arr.append(f'[{token_str}]{true_labels[i]}, ')\n\n    if is_print_details:\n        # Выводим просто строку в контексте\n        out = ''\n        for el in token_arr:\n            out += el\n        print(out)\n        print()\n        # Выводим просто токены строки с тру метками\n        out = ''\n        for el in label_arr:\n            out += el\n        print(out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n#if 1==2:\n    # Добавим к df_result метки к токенам с персональной ифонманицией, которые нашла модель\n    for row in df.itertuples():\n        \n        # Находим индекс документа в датасете\n        index_to_change = df_result.index[df_result['document'] == row.document][0]\n        df_result.loc[index_to_change, 'pred_labels'][row.token] = row.label\n        # df_result.loc[index_to_change, 'pred_token_str'][row.token] = row.token_str\n        # print(doc_index, row.token, row.document, row.label)\n        #break\n    labels = [\n        'B-EMAIL',\n        'B-ID_NUM',\n        'B-NAME_STUDENT',\n        'B-PHONE_NUM',\n        'B-STREET_ADDRESS',\n        'B-URL_PERSONAL',\n        'B-USERNAME',\n        'I-ID_NUM',\n        'I-NAME_STUDENT',\n        'I-PHONE_NUM',\n        'I-STREET_ADDRESS',\n        'I-URL_PERSONAL',\n        'O'\n    ]\n    \n    processed_result = []  # Список для обработанных записей\n    \n    metrics = {\n        'FP': 0, # Ошибочно пометил токен что персональная ифнормация\n        'TP': 0, # Проавильно пометил токен с персональной информацией\n        'FN': 0, # Не пометил токен с персональной информацией\n        'TN': 0, # Проавильно пометил токен с не персональной информацией\n        'err_dist': 0, # Нашел что токен персональная ифонмация но не того типа\n        'err_all': 0, # Всего ошибок\n        'torekn_all': 0 # Всего токенов\n    }\n\n    # Число ошибок в разрезе отдельных меток\n    label_err_count = {key: 0 for key in labels}\n    # Инициализируем число истинных токенов в разрезе по меткам\n    true_label_count = {key: 0 for key in labels}\n    # Инициализируем минимальную длину истинных токенов в разрезе по меткам\n    true_label_min_len = {key: 100 for key in labels}\n    name_str = []\n\n    \n    # Пройдемся по предсказаниям и посмотрим где они не совпадают с истинными метками токенов.\n    # Там где на совпадает \n    if is_print_details:\n        print('Не совпадение предсказаний с истинными метками токенов:\\n')\n    \n    doc_idx = 0 # Индекс документа\n    for tokens, doc, true_labels, pred_labels, pred_indes in zip(\n        df_result[\"tokens\"],\n        df_result[\"document\"],\n        df_result[\"true_labels\"],\n        df_result[\"pred_labels\"],\n        df_result[\"pred_ind\"]):\n    \n        # Номер токена\n        token_id = 0\n\n        #Сколько токенов в документе\n        true_labels_len = len(true_labels)\n\n        # Итерируемся по каждому прогнозу токена и соответствующим смещениям\n        for token, true_label, pred_label in zip(tokens, true_labels, pred_labels):\n            metrics['torekn_all'] += 1\n            true_label_count[true_label] += 1\n            # Находим минимальную длину истинного токена\n            # При условии что дальше нет продолжения\n            is_next_token = False\n            if token_id + 1 < len(tokens):\n                if bool(re.search(r'^I-', true_labels[token_id + 1])):\n                    is_next_token = True\n            if not(is_next_token):\n                true_label_min_len[true_label] = len(token) if len(token) < true_label_min_len[true_label] else true_label_min_len[true_label]\n                if true_label == 'B-NAME_STUDENT':\n                    if len(token) <=2:\n                        #name_str.append([doc,token_id,len(token),token])\n                        pass\n            if true_label == 'B-NAME_STUDENT':\n                name_str.append([doc,token_id,len(token),token])\n            # if (true_label not in (\"O\")) or (pred_label not in (\"O\")):\n            if not(true_label == pred_label):\n                # если определеили токен не правильно\n                \n                metrics['err_all'] += 1\n                label_err_count[pred_label] += 1\n                \n                if is_print_details: \n                    print(f'\\n******************************************** {metrics[\"err_all\"]} ********************************************\\n')\n                    print(pd.DataFrame({\"document\": [doc],\n                                        \"doc_len\": [true_labels_len],\n                                        \"token\": [token_id],\n                                        #\"true_label\": [true_label],\n                                        \"true_label-pred_label-len\": [f'{true_label} {pred_label} {len(token)}'],\n                                        #\"token_len\": [len(token)],\n                                        \"token_str\": [token]})\n                          .to_string(index=False))\n                # Печатаем фрагмент текста c токеном по середине который не правильно определили\n                print_fragment()\n\n                # pred_ind -индекс текущего токена в токенах предсказания (токены модели и тестовые токены могут отличаться в том числе по номеру в тексте)\n                pred_ind = pred_indes[token_id] \n\n                if pred_ind > 0:\n                    \n                    # Создаем датафрейм где будут предсказания моделей по меткам\n                    pred_mon_df = pd.DataFrame({'Labels': labels})\n                    #if is_print_details:\n                    if 1 == 2:\n                        # Выведем предсказания всех моделей для всех возможных классов для текущего токена\n                        for model_idx in range(len(model_predictions)):\n                            pred_mon_df[f'model_{model_idx}'] = [round(num * 100) \n                                                                 for num in model_predictions[model_idx][doc_idx][pred_ind]]\n                        pred_mon_df[f'av_pred'] = [round(num * 100)\n                                                   for num in weighted_average_predictions[doc_idx][pred_ind]]\n                        display(pred_mon_df)\n                else:\n                    if is_print_details:\n                        print('Не нашел индекс предсказания для токена!!!')\n            else:\n                # если определеили токен правильно\n                if pred_label == 'O':\n                    metrics['TN'] += 1\n                else:\n                    metrics['TP'] += 1\n            token_id += 1\n        doc_idx += 1\n    # Считаем точность\n    try:\n        metrics['precision'] = metrics['TP']/(metrics['TP'] + metrics['FP'])\n    except:\n        metrics['precision'] = 0.0\n    \n    # Считаем полноту\n    try:\n        metrics['recall'] = metrics['TP']/(metrics['TP'] + metrics['FN'])\n    except:\n        metrics['recall'] = 0.0\n    \n    # Считаем F5\n    try:\n        metrics['F5'] = (26 * metrics['precision'] * metrics['recall']) / (25 * metrics['precision'] + metrics['recall'])\n    except:\n        metrics['F5'] = 0.0\n\n    print('\\n------------------------------------------------------------------------------------\\n')\n    print('Минимальные длины токенов:')\n    display(true_label_min_len)\n    \n    print('\\n------------------------------------------------------------------------------------\\n')\n    print('Всего истинных токенов в разрезе по меткам:')\n    display(true_label_count)\n    \n    print('\\n------------------------------------------------------------------------------------\\n')\n    print('Ошибкок предсказний модели в разрезе по меткам:')\n    display(label_err_count)\n    \n    print('\\n------------------------------------------------------------------------------------\\n')\n    print ('Всего токенов:', metrics['torekn_all'])\n    print ('Не отметил токены с персональной информацией (FN):', metrics['FN'])\n    print ('Пометил лишних токенов (FP):', metrics['FP'])\n    print ('Не правильно определил тип персональной информации:', metrics['err_dist'])\n    print ('precision:', metrics['precision'])\n    print ('recall:', metrics['recall'])\n    print ('F5:', metrics['F5'])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    cols = ['doc', 'token', 'len', 'token_str']\n    name_str_df = pd.DataFrame(name_str, columns=cols)\n    name_str_df_grouped = name_str_df.groupby('doc')['token_str'].count()\n    display(name_str_df_grouped.value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    for tokens, doc, true_labels, pred_labels, pred_indes in zip(\n        df_result[\"tokens\"],\n        df_result[\"document\"],\n        df_result[\"true_labels\"],\n        df_result[\"pred_labels\"],\n        df_result[\"pred_ind\"]):\n        true_labels_len = len(true_labels)\n        for idx, (token, true_label, pred_label) in enumerate(zip(tokens, true_labels, pred_labels)):\n            if true_label != 'O':\n                print ('doc:',doc, 'true_label:',true_label, 'token idx:', idx, 'tokens all:', true_labels_len)\n        ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(p_time(), 'end notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    for tokens, doc, true_labels, pred_labels, pred_indes in zip(\n        df_result[\"tokens\"],\n        df_result[\"document\"],\n        df_result[\"true_labels\"],\n        df_result[\"pred_labels\"],\n        df_result[\"pred_ind\"]):\n        true_labels_len = len(true_labels)\n        for idx, (token, true_label, pred_label) in enumerate(zip(tokens, true_labels, pred_labels)):\n            if true_label != 'O':\n                print ('doc:',doc, 'true_label:',true_label, 'token idx:', idx, 'tokens all:', true_labels_len)\n                ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    for tokens, doc, true_labels, pred_labels, pred_indes in zip(\n        df_result[\"tokens\"],\n        df_result[\"document\"],\n        df_result[\"true_labels\"],\n        df_result[\"pred_labels\"],\n        df_result[\"pred_ind\"]):\n        true_labels_len = len(true_labels)\n        for idx, (token, true_label, pred_label) in enumerate(zip(tokens, true_labels, pred_labels)):\n            if true_label != 'O' or pred_label != 'O':\n                print ('doc:',doc, 'token:', token, ', true_label:',true_label, 'pred_label:',pred_label, 'token idx:', idx, 'tokens all:', true_labels_len)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_local:\n    print(test_data[doc_list.index(9405)]['full_text'])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}